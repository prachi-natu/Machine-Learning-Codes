{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Machine Learning </center>\n",
    "### <center> Experiment No. 4.1: Simple Linear Regression using Gradient Descent </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aim:** Implement Gradient Descent Algorithm in Linear regression using Numpy.\n",
    "\n",
    "**Prerequisites:** Concepts of linear regression, Gradient Descent Algorithm\n",
    "\n",
    "**Outcome:** After completion of this experiment students will be able to build linear regression model on simple dataset using  Gradient Descent Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\n",
    "When there is a single input variable (x), the method is referred to as simple linear regression.\n",
    "\n",
    "Different techniques can be used to prepare or train the linear regression equation from data, the most common of which is called Ordinary Least Squares. Both the input values (x) and the output value are numeric. The linear equation assigns one scale factor to each input value or column, called a coefficient and represented by the capital Greek letter Beta (B). One additional coefficient is also added, giving the line an additional degree of freedom (e.g. moving up and down on a two-dimensional plot) and is often called the intercept or the bias coefficient.\n",
    "\n",
    "For example, in a simple regression problem (a single x and a single y), the form of the model would be: y(pred) = B0 + B1*x.\n",
    "\n",
    "In higher dimensions when we have more than one input (x), the line is called a plane or a hyper-plane. The representation therefore is the form of the equation and the specific values used for the coefficients. When a coefficient becomes zero, it effectively removes the influence of the input variable on the model and therefore from the prediction made from the model (0 * x = 0).\n",
    "\n",
    "Learning a linear regression model means estimating the values of the coefficients used in the representation with the data that we have available.\n",
    "The core idea is to obtain a line that best fits the data. The best fit line is the one for which total prediction error (all data points) are as small as possible. Error is the distance between the point to the regression line.\n",
    "\n",
    "For simple linear regression, y(pred) = b0 + b1*x. The values b0 and b1 must be chosen so that they minimize the error. If sum of squared error is taken as a metric to evaluate the model, then goal to obtain a line that best reduces the error.\n",
    "\n",
    "If the model does not include x i.e. x=0, then the prediction will become meaningless with only b0. For example, we have a dataset that relates height(x) and weight(y). Taking x=0(that is height as 0), will make equation have only b0 value which is completely meaningless as in real-time height and weight can never be zero. This resulted due to considering the model values beyond its scope.\n",
    "\n",
    "If the model includes value 0, then ‘b0’ will be the average of all predicted values when x=0. But, setting zero for all the predictor variables is often impossible.\n",
    "The value of b0 guarantee that residual have mean zero. If there is no ‘b0’ term, then regression will be forced to pass over the origin. Both the regression co-efficient and prediction will be biased.\n",
    "\n",
    "When we have more than one input we can use Ordinary Least Squares to estimate the values of the coefficients.\n",
    "\n",
    "The Ordinary Least Squares procedure seeks to minimize the sum of the squared residuals. This means that given a regression line through the data we calculate the distance from each data point to the regression line, square it, and sum all of the squared errors together. This is the quantity that ordinary least squares seeks to minimize.\n",
    "\n",
    "**Gradient Descent**\n",
    "\n",
    "Gradient Descent is the process of minimizing a function by following the gradients of the cost function.\n",
    "\n",
    "This involves knowing the form of the cost as well as the derivative so that from a given point you know the gradient and can move in that direction, e.g. downhill towards the minimum value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function\n",
    "\n",
    "It turns out that to make the best line to model the data, we want to pick parameters $\\beta$ that allows our predicted value to be as close to the actual value as possible. In other words, we want the distance or residual between our hypothesis $h(x)$ and y to be minimized.\n",
    "\n",
    "So we formally define a cost function using ordinary least squares that is simply the sum of the squared distances. To find the liner regression line, we minimize:$$J(\\beta) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\beta(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "Again the hypothesis that we're trying to find is given by the linear model:$$h_\\beta(x) = \\beta^{T}x = \\beta_0 + \\beta_1x_1$$\n",
    "\n",
    "The parameters of the model are the beta values. We adjust $\\beta_j$ to minimze the cost function $J(\\beta)$.\n",
    "\n",
    "And we can use batch gradient descent where each iteration performs the update$$\\beta_j := \\beta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m (h_\\beta(x^{(i)})-y^{(i)})x_{j}^{(i)}$$\n",
    "\n",
    "Whoa, what's gradient descent? And why are we updating that?\n",
    "\n",
    "Gradient descent simply is an algorithm that makes small steps along a function to find a local minimum. We can look at a simply quadratic equation such as this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file with columns 'population' and 'profit'\n",
    "data = pd.read_csv('ex1data1.txt', names = ['population', 'profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints data\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split population and profit into X and y and plot data\n",
    "X_df = pd.DataFrame(data.population)\n",
    "y_df = pd.DataFrame(data.profit)\n",
    "\n",
    "## Length, or number of observations, in our data\n",
    "m = len(y_df)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(X_df, y_df, 'kx')\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1500\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a columns of 1s as intercept to X\n",
    "X_df['intercept'] = 1\n",
    "\n",
    "## Transform to Numpy arrays for easier matrix math and start theta at 0\n",
    "X = np.array(X_df)\n",
    "y = np.array(y_df).flatten()\n",
    "theta = np.array([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, y, theta):\n",
    "    \"\"\"\n",
    "    cost_function(X, y, theta) computes the cost of using theta as the\n",
    "    parameter for linear regression to fit the data points in X and y\n",
    "    \"\"\"\n",
    "    ## number of training examples\n",
    "    m = len(y) \n",
    "    \n",
    "    ## Calculate the cost with the given parameters\n",
    "    J = np.sum((X.dot(theta)-y)**2)/2/m\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function(X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    gradient_descent Performs gradient descent to learn theta\n",
    "    theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by \n",
    "    taking num_iters gradient steps with learning rate alpha\n",
    "    \"\"\"\n",
    "    cost_history = [0] * iterations\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        hypothesis = X.dot(theta)\n",
    "        loss = hypothesis-y\n",
    "        gradient = X.T.dot(loss)/m\n",
    "        theta = theta - alpha*gradient\n",
    "        cost = cost_function(X, y, theta)\n",
    "        cost_history[iteration] = cost\n",
    "\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(theta, c) = gradient_descent(X,y,theta,alpha, iterations)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "print np.array([3.5, 1]).dot(t)\n",
    "print np.array([7, 1]).dot(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the best fit line\n",
    "best_fit_x = np.linspace(0, 25, 20)\n",
    "best_fit_y = [t[1] + t[0]*xx for xx in best_fit_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(X_df.population, y_df, '.')\n",
    "plt.plot(best_fit_x, best_fit_y, '-')\n",
    "plt.axis([0,25,-5,25])\n",
    "plt.xlabel('Population of City in 10,000s')\n",
    "plt.ylabel('Profit in $10,000s')\n",
    "plt.title('Profit vs. Population with Linear Regression Line')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
